{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37f8523f-f7bc-4ffe-bc55-11290b103415",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\adisu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import pathlib\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Input\n",
    "import matplotlib.pyplot as plt\n",
    "# Membuat objek stemmer dari Sastrawi\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "stop_words = set(stopwords.words('indonesian'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5175f0ee-8efd-4b7a-8f3c-39c8e2baadcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Menghapus karakter non-kata dan menurunkan huruf\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Menghapus stopwords\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "    # Melakukan stemming dengan Sastrawi\n",
    "    text = ' '.join([stemmer.stem(word) for word in text.split()])\n",
    "\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62794e08-feef-493b-8ed4-02e9c7d0a272",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['processed_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['processed_text'])\n",
    "\n",
    "X = tokenizer.texts_to_sequences(df['processed_text'])\n",
    "X = pad_sequences(X, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67e31fcd-4de8-4eec-a124-59a6a061bf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_word2vec = Word2Vec(df['processed_text'], vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9405c90-b64f-4c32-becd-2b1232d69c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18ad9896-8e1c-4181-91be-62b822a6c212",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in model_word2vec.wv:\n",
    "        embedding_matrix[i] = model_word2vec.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7f74a49-32f8-4ff7-b426-66982f5f4f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['label']\n",
    "train_size = int(len(X) * 0.8)\n",
    "\n",
    "X_train = X[:train_size]\n",
    "X_test = X[train_size:]\n",
    "y_train = y[:train_size]\n",
    "y_test = y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b91d90e-c611-4774-b843-4cbe6d758107",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(input_dim=len(tokenizer.word_index) + 1,\n",
    "              output_dim=embedding_dim,\n",
    "              weights=[embedding_matrix],\n",
    "              trainable=True),\n",
    "    GRU(64, return_sequences=True, activation='relu'),\n",
    "    GRU(32, return_sequences=False, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(8, activation='softmax')  # 8 kelas emosi\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "397e83d1-41ea-46c1-a553-81c7c29ec038",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08550cb7-c1c4-4de2-877c-0afda7680a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.1378 - loss: 2.0543 - val_accuracy: 0.2587 - val_loss: 1.7959\n",
      "Epoch 2/15\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.3577 - loss: 1.6112 - val_accuracy: 0.7613 - val_loss: 0.7452\n",
      "Epoch 3/15\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.6897 - loss: 0.8350 - val_accuracy: 0.8875 - val_loss: 0.4243\n",
      "Epoch 4/15\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8391 - loss: 0.5058 - val_accuracy: 0.9112 - val_loss: 0.3507\n",
      "Epoch 5/15\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8663 - loss: 0.4054 - val_accuracy: 0.9212 - val_loss: 0.3274\n",
      "Epoch 6/15\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9163 - loss: 0.2764 - val_accuracy: 0.9287 - val_loss: 0.2790\n",
      "Epoch 7/15\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9249 - loss: 0.2197 - val_accuracy: 0.9337 - val_loss: 0.3263\n",
      "Epoch 8/15\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9433 - loss: 0.1863 - val_accuracy: 0.9225 - val_loss: 0.4903\n",
      "Epoch 9/15\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9393 - loss: 0.1966 - val_accuracy: 0.9038 - val_loss: 0.5404\n",
      "Epoch 10/15\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9538 - loss: 0.1426 - val_accuracy: 0.9388 - val_loss: 0.3533\n",
      "Epoch 11/15\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9457 - loss: 0.1615 - val_accuracy: 0.9175 - val_loss: 0.2803\n",
      "Epoch 12/15\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.9574 - loss: 0.1384 - val_accuracy: 0.9400 - val_loss: 0.3200\n",
      "Epoch 13/15\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.9615 - loss: 0.1378 - val_accuracy: 0.9300 - val_loss: 0.3291\n",
      "Epoch 14/15\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9740 - loss: 0.0749 - val_accuracy: 0.9350 - val_loss: 0.3939\n",
      "Epoch 15/15\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9771 - loss: 0.0845 - val_accuracy: 0.9312 - val_loss: 0.4613\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x159a8c02c90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=15,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adec710a-a617-4ec1-83d0-8ecc60cd5699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9088 - loss: 0.6156\n",
      "Model Accuracy: 0.9312\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Model Accuracy: {accuracy:.4f}')\n",
    "model.save(\"emotion_prediction.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "692ac831-9e7c-4919-8ee8-4933c6d9c953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict_emotion(text, top_n=3):\n",
    "    # Preprocess the input text\n",
    "    processed_text = preprocess_text(text)\n",
    "\n",
    "    # Convert the text into a sequence of integers\n",
    "    sequence = tokenizer.texts_to_sequences([processed_text])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=X.shape[1], padding='post')\n",
    "\n",
    "    # Get the model's predictions\n",
    "    prediction = model.predict(padded_sequence)\n",
    "\n",
    "    # Get the indices of the top N emotions based on prediction probabilities\n",
    "    top_indices = np.argsort(prediction[0])[::-1][:top_n]\n",
    "\n",
    "    # Define the possible emotions\n",
    "    emotions = ['Waspada', 'Marah', 'Jijik', 'Takut', 'Senang', 'Sedih', 'Terkejut', 'Percaya']\n",
    "\n",
    "    # Get the top N emotions with their corresponding probabilities\n",
    "    top_emotions = [(emotions[i], prediction[0][i]) for i in top_indices]\n",
    "\n",
    "    return top_emotions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2802ea9b-d575-4788-a955-6d755031aef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 673ms/step\n",
      "Teks: Tiba-tiba aku menerima panggilan dari teman lama yang sudah lama tidak berhubungan, itu sangat mengejutkan!\n",
      "Emosi yang diprediksi: [('Terkejut', 1.0)]\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Teks: Saya sangat marah karena pertemuan tadi pagi sangat tidak produktif. Semua ide yang saya usulkan ditolak begitu saja!\n",
      "Emosi yang diprediksi: [('Marah', 1.0)]\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Teks: Aku merasa jijik ketika melihat sampah berserakan di lantai rumah makan.\n",
      "Emosi yang diprediksi: [('Jijik', 1.0)]\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Teks: Aku merasa takut berjalan sendirian di jalan sepi malam ini.\n",
      "Emosi yang diprediksi: [('Takut', 0.9999558)]\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Teks: Hari ini sangat menyenankan! Saya bertemu teman lama dan kami menghabiskan waktu bersama di kafe. Rasanya seperti kembali ke masa-masa indah!\n",
      "Emosi yang diprediksi: [('Marah', 0.8421451)]\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Teks: Hari ini aku merasa sangat sedih, rasanya seperti dunia ini tidak adil\n",
      "Emosi yang diprediksi: [('Sedih', 1.0)]\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Teks: Aku sangat terkejut ketika tahu bahwa teman dekatku sudah menikah tanpa memberitahuku.\n",
      "Emosi yang diprediksi: [('Terkejut', 0.9999223)]\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Teks: Aku percaya bahwa dengan kerja keras dan doa, aku akan berhasil mencapai impian.\n",
      "Emosi yang diprediksi: [('Percaya', 1.0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "teks_emosi=['Tiba-tiba aku menerima panggilan dari teman lama yang sudah lama tidak berhubungan, itu sangat mengejutkan!', # Waspada\n",
    "    'Saya sangat marah karena pertemuan tadi pagi sangat tidak produktif. Semua ide yang saya usulkan ditolak begitu saja!', # Marah\n",
    "    'Aku merasa jijik ketika melihat sampah berserakan di lantai rumah makan.', #Jijik\n",
    "    'Aku merasa takut berjalan sendirian di jalan sepi malam ini.', #Takut\n",
    "    'Hari ini sangat menyenankan! Saya bertemu teman lama dan kami menghabiskan waktu bersama di kafe. Rasanya seperti kembali ke masa-masa indah!', #Senang\n",
    "    'Hari ini aku merasa sangat sedih, rasanya seperti dunia ini tidak adil', #Sedih\n",
    "    'Aku sangat terkejut ketika tahu bahwa teman dekatku sudah menikah tanpa memberitahuku.', #Terkejut\n",
    "    'Aku percaya bahwa dengan kerja keras dan doa, aku akan berhasil mencapai impian.' #Percaya\n",
    "]\n",
    "\n",
    "for text in teks_emosi:\n",
    "    emotion = predict_emotion(text, top_n=1)\n",
    "    print(f\"Teks: {text}\")\n",
    "    print(f\"Emosi yang diprediksi: {emotion}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4178d56-d366-4618-9646-9086b6a6c634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/assets\n"
     ]
    }
   ],
   "source": [
    "export_dir = 'saved_model/'\n",
    "tf.saved_model.save(model, export_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9e03e02-ab56-4ab2-a372-fc5300e23e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"emotion_prediction.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c659072-8931-4856-b1a1-725cee8c24b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer telah disimpan!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Simpan Tokenizer ke dalam file dengan Pickle\n",
    "with open('tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "print(\"Tokenizer telah disimpan!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d02d14ea-2d55-4f10-8452-9ff8b6db13f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)\n",
    "tflite_model = converter.convert()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c43ee923-13c7-45e4-9117-31c8592e7094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126492"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_model_file = pathlib.Path('model.tflite')\n",
    "tflite_model_file.write_bytes(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f34840-23ac-4a21-bafc-7b026859513f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
